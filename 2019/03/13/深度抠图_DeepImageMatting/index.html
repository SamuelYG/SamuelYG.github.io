<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 3.8.0">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon.png">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css">


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    hostname: new URL('https://blog.samuelyg.cn').hostname,
    root: '/',
    scheme: 'Pisces',
    version: '7.7.0',
    exturl: false,
    sidebar: {"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},
    copycode: {"enable":false,"show_result":false,"style":null},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: true,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    comments: {"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    path: 'search.xml',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}
  };
</script>

  <meta name="description" content="本文是 Adobe 等机构的论文 Deep Image Matting 译文。因为是初稿，如有错误及不当之处，烦请指正。">
<meta name="keywords" content="深度抠图,论文">
<meta property="og:type" content="article">
<meta property="og:title" content="深度抠图 Deep Image Matting 译文">
<meta property="og:url" content="https://blog.samuelyg.cn/2019/03/13/深度抠图_DeepImageMatting/index.html">
<meta property="og:site_name" content="YangGang&#39;Blog">
<meta property="og:description" content="本文是 Adobe 等机构的论文 Deep Image Matting 译文。因为是初稿，如有错误及不当之处，烦请指正。">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="https://miao.su/images/2019/03/13/15524836698559c297.png">
<meta property="og:image" content="https://miao.su/images/2019/03/13/1552483942593f2efa.png">
<meta property="og:image" content="https://miao.su/images/2019/03/13/1552372427134b4b5c.png">
<meta property="og:image" content="https://miao.su/images/2019/03/13/155237991322231706.png">
<meta property="og:image" content="https://miao.su/images/2019/03/13/1552392001716977c3.png">
<meta property="og:image" content="https://miao.su/images/2019/03/13/155239233423197481.png">
<meta property="og:image" content="https://miao.su/images/2019/03/13/1552392382192d4998.png">
<meta property="og:image" content="https://miao.su/images/2019/03/13/155239239608952146.png">
<meta property="og:image" content="https://miao.su/images/2019/03/13/1552392408598fb05f.png">
<meta property="og:image" content="https://miao.su/images/2019/03/13/15523924257746ddd9.png">
<meta property="og:image" content="https://miao.su/images/2019/03/13/1552392440095e5bf4.png">
<meta property="og:image" content="https://miao.su/images/2019/03/13/1552392578792b84c6.png">
<meta property="og:image" content="https://miao.su/images/2019/03/13/1552392592015c7238.png">
<meta property="og:image" content="https://miao.su/images/2019/03/13/15523926030398e34a.png">
<meta property="og:image" content="https://miao.su/images/2019/03/13/15523926142868c7ce.png">
<meta property="og:image" content="https://miao.su/images/2019/03/13/15523926259836c4b0.png">
<meta property="og:image" content="https://miao.su/images/2019/03/13/155239263790389b01.png">
<meta property="og:image" content="https://miao.su/images/2019/03/13/15523994212486ce95.png">
<meta property="og:image" content="https://miao.su/images/2019/03/13/15524000053679b0b5.png">
<meta property="og:image" content="https://miao.su/images/2019/03/13/15524023237505c1e7.png">
<meta property="og:updated_time" content="2019-06-05T05:03:51.819Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="深度抠图 Deep Image Matting 译文">
<meta name="twitter:description" content="本文是 Adobe 等机构的论文 Deep Image Matting 译文。因为是初稿，如有错误及不当之处，烦请指正。">
<meta name="twitter:image" content="https://miao.su/images/2019/03/13/15524836698559c297.png">

<link rel="canonical" href="https://blog.samuelyg.cn/2019/03/13/深度抠图_DeepImageMatting/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true
  };
</script>

  <title>深度抠图 Deep Image Matting 译文 | YangGang'Blog</title>
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-98889996-1"></script>
    <script>
      if (CONFIG.hostname === location.hostname) {
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'UA-98889996-1');
      }
    </script>






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">YangGang'Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
  </div>

  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档</a>

  </li>
  </ul>

</nav>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block " lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://blog.samuelyg.cn/2019/03/13/深度抠图_DeepImageMatting/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://miao.su/images/2017/03/03/dark_deer302fb.png">
      <meta itemprop="name" content="YangGang">
      <meta itemprop="description" content="(ง •_•)ง">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="YangGang'Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          深度抠图 Deep Image Matting 译文
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2019-03-13 11:00:00" itemprop="dateCreated datePublished" datetime="2019-03-13T11:00:00+08:00">2019-03-13</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2019-06-05 13:03:51" itemprop="dateModified" datetime="2019-06-05T13:03:51+08:00">2019-06-05</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/译文/" itemprop="url" rel="index">
                    <span itemprop="name">译文</span>
                  </a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/译文/CV/" itemprop="url" rel="index">
                    <span itemprop="name">CV</span>
                  </a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>本文是 Adobe 等机构的论文 <a href="https://arxiv.org/abs/1703.03872" target="_blank" rel="noopener">Deep Image Matting</a> 译文。因为是初稿，如有错误及不当之处，烦请指正。</p>
<a id="more"></a>
<h1 id="深度抠图"><a href="#深度抠图" class="headerlink" title="深度抠图"></a>深度抠图</h1><center>Ning Xu<sup>1,2</sup>, Brain Price<sup>3</sup>, Scott Cohen<sup>3</sup>, and Thomas Huang<sup>1,2</sup></center>



<center><sup>1</sup>Beckman Institute for Advanced Science and Technology</center>

<center><sup>2</sup>University of Illinois at Urbana-Champaign</center>

<center><sup>3</sup>Adobe Research</center>

<center>{ningxu2, t-huang1}@illinois.edu, {bprice, scohen}@adobe.com</center>

<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>抠图是一个基本的计算机视觉问题，并且拥有广阔的应用空间。但当图像的前景和背景颜色或复杂纹理类似时，以前的算法表现的差强人意。主要是因为：1. 只有低层次特征（low-level features）；2. 缺乏高层次上下文图镜（high-level context）。本文中，我们提出了一种基于深度学习的新算法，该算法可以解决这两个问题。我们的深度模型分为两个阶段：第一个阶段是深度卷积编码-解码网络（deep convolutional encoder-decoder network），该神经网络将图像和对应的三分图（trimap）作为输入，并预测图像的 α 蒙版（α matte）；第二阶段是一个小型卷积神经网络，该神经网络对第一个网络预测的 α 蒙版进行精炼从而拥有更准确的 α 值和锐化边缘。另外，我们还创建了一个大规模抠图数据集（large-scale image matting dataset），该数据集包含 49300 张训练图像和 1000 张测试图像。我们在抠图基准、测试数据集和各种真实图像上评估了我们的算法。实验结果清楚的表明了我们的算法比先前的方法更具优越性。</p>
<h2 id="1-简介"><a href="#1-简介" class="headerlink" title="1. 简介"></a>1. 简介</h2><p>抠图是图像或视频中准确的前景估计问题，具有重要的现实意义。它是图像编辑和电影制作的关键技术，高效的自然图像抠图方法能极大地提升当前的图像视频处理流程的效率。并且这种技术是无约束场景（unconstrained scenes）下处理真实世界图像的必要方法。</p>
<p>不幸的是，目前的抠图方法无法很好的推广到典型的日常场景。这在一定程度上是由于问题的难度：抠图问题是欠约束的，在每个像素上有 7 个未知值却仅有 3 个已知值：</p>
<script type="math/tex; mode=display">
I_i = α_iF_i + (1 - α_i)B_i \quad α_i∈[0, 1] \tag {1}</script><p>其中已知的 <script type="math/tex">I_i</script> 是像素 <script type="math/tex">i</script> 的 RGB 色彩，前景色 <script type="math/tex">F_i</script> ，背景色 <script type="math/tex">B_i</script> 和蒙版估计（matte estimation）<script type="math/tex">α_i</script> 均为未知量。因此，目前的方法在其方法中进一步受到限制。</p>
<p>第一个限制是目前用来求解抠图方程（matting equation）的方法存在问题。</p>
<p>这个方程将抠图问题表述为两种颜色的线性组合，因此目前大多数算法都将其作为颜色问题来处理。标准方法包括采样前景色和背景色 <sup><a href="#fn_3" id="reffn_3">3</a></sup><sup><a href="#fn_9" id="reffn_9">9</a></sup> ，根据抠图方程 <sup><a href="#fn_14" id="reffn_14">14</a></sup><sup><a href="#fn_31" id="reffn_31">31</a></sup><sup><a href="#fn_22" id="reffn_22">22</a></sup> 或者两者的混合 <sup><a href="#fn_32" id="reffn_32">32</a></sup><sup><a href="#fn_13" id="reffn_13">13</a></sup><sup><a href="#fn_28" id="reffn_28">28</a></sup><sup><a href="#fn_16" id="reffn_16">16</a></sup> 传播 <script type="math/tex">α</script> 值。这种方法很大程度上依赖于颜色作为区别特征（通常与像素的空间位置一起），使得它们对前景和背景颜色分布重叠的情况非常敏感，对于这个方法不幸的是，自然图像的常见情况，根据方法的不同，往往导致低频“smearing”或高 频“chunky”伪影（参见图 <a href="#1">1</a> 的顶行）。即使是最近提出的深度学习方法也高度依赖于颜色相关的传播方法<sup><a href="#fn_8" id="reffn_8">8</a></sup><sup><a href="#fn_29" id="reffn_29">29</a></sup>。</p>
<p><img src="https://miao.su/images/2019/03/13/15524836698559c297.png" alt="图像|三分图|Closed-form|Ours"></p>
<blockquote>
<p>图 <a href="#1">1</a>. 我们的方法和 Close-form Matting<sup><a href="#fn_22" id="reffn_22">22</a></sup>之间的比较第一张图来自 Alpha Matting 基准，第二张图片来自我们的 1000 张测试图像</p>
</blockquote>
<p>第二个限制是由于非常小的数据集。一般用于抠图的标准结果（ground truth）是很复杂的，而 <a href="http://alphamatting.com/" target="_blank" rel="noopener">alphamatting.com</a> 数据集 <sup><a href="#fn_25" id="reffn_25">25</a></sup> 通过提供标记数据集（ground-truth）对抠图研究做出了很重要的贡献。不过因为该数据集只由 27 张训练图像和 8 张测试图像组成，其中大部分是监视器上图像前面的物体。由于其大小和数据集的约束（例如：室内实验室场景，室内照明，没有人类或动物），它本质上会存在偏差，并且为了发布的目的激励方法适应这些数据。与所有数据集（尤其是小数据集）的情况一样，某些时候，方法将过拟合数据集，不再能推广到真实场景。最近拥有 3 个训练视频和 10 个测试视频的视频抠图数据集<sup><a href="#fn_10" id="reffn_10">10</a></sup> ，其中 5 个是从绿屏画面中提取的，其余的是使用类似alphamatting数据集<sup><a href="#fn_25" id="reffn_25">25</a></sup>的方法。</p>
<p>在这项工作中，我们提供了一种旨在克服这些局限性的方法。我们的方法是使用深度学习在给定输入图像和三分图的基础上直接预测 α 蒙版（alpha matte）。我们的神经网络并不首要依赖于色彩信息，它会学习图像的自然结构，并将其反映到 α 蒙版中。例如头发和毛皮（通常需要将其抠出来）就拥有很强的结构和纹理图案。以及其他通常需要抠图的并且存在能抽取出的公用结构体或 <script type="math/tex">α</script> 蒙版轮廓的情况（例如：物体的边缘、光学或运动模糊的区域、或者半透明区域）。并且由于低层次的特征并不会捕获这些结构，那么就需要深度神经网络去表征它们了。我们的两阶段神经网络包含了编码器-解码器阶段和使用小型残差网络进行精炼阶段，并且包含除了 <script type="math/tex">α</script> 损失之外的新的合成损失（ompositional loss）。我们是第一个证明了在给定输入图像和三分图的情况下能采用端到端的方式学习到 <script type="math/tex">α</script> 蒙版。</p>
<p>为了训练一个在无约束场景的自然图像中表现优异的模型，我们需要一个比现有更大的数据集。使用alphamatting <sup><a href="#fn_25" id="reffn_25">25</a></sup> 的方法获得标准结果（ground truth）数据集是非常复杂的，并且无法处理任何程度的运动场景（因此无法捕获人类或动物）。相反，受其他合成数据集已被证明足以训练模型用于真实图像的启发（例如：<sup><a href="#fn_4" id="reffn_4">4</a></sup>），我们使用合成方法构建了一个大规模的抠图数据集。仔细提取具有简单背景图片上的对象并将其合成为具有新的背景的图像以创建具有 49300 个训练图像和 1000 个测试图像的数据集。</p>
<p>我们进行了广泛的评估，以证明我们的方法的有效性。我们的方法不仅在 <a href="http://alphamatting.com/" target="_blank" rel="noopener">alphamatting.com</a> 挑战中获得第一名，而且我们在合成测试集上的表现也大大超过了先前的方法。我们与用户展示并研究了我们的学习模型推广（generalizes）到自然图像的情况并与先前的许多方法在 31 种不同场景、不同照明并以人类、动物和其他物体为特色的自然图像中的表现做了对比。研究显示更偏向于我们的结果，但同时也表明，与人类判断的的其他方法相比，某些在 <a href="http://alphamatting.com/" target="_blank" rel="noopener">alphamatting.com</a> 数据集上表现良好的方法实际上表现的较差，这表明这些方法在 <a href="http://alphamatting.com/" target="_blank" rel="noopener">alphamatting.com</a> 数据集上是过拟合的。最后，模型还展示出对 trimap 的放置比其他方法更稳健。实际上，即使 trimap 没有已知的前景和/或背景，我们也可以得到很好的结果，而大多数方法都不能返回任何结果（参见图 1第2行）。</p>
<h2 id="2-相关工作"><a href="#2-相关工作" class="headerlink" title="2. 相关工作"></a>2. 相关工作</h2><p>目前的抠图方法主要依靠颜色以及位置或其他低层次特征来确定 <script type="math/tex">α</script> 蒙版（alpha matte）。它们通过采样、传播或两者的组合来实现。</p>
<p>基于采样的方法 <sup><a href="#fn_3" id="reffn_3">3</a></sup><sup><a href="#fn_9" id="reffn_9">9</a></sup><sup><a href="#fn_32" id="reffn_32">32</a></sup><sup><a href="#fn_13" id="reffn_13">13</a></sup><sup><a href="#fn_28" id="reffn_28">28</a></sup><sup><a href="#fn_16" id="reffn_16">16</a></sup> 中，对已知的前景和背景区域采样来找到对于给定像素的前景和背景候选颜色，然后使用度量（metric）来确定最佳前景/背景组合。使用不同的采样方法，包括距离给定像素最近的边界进行采样 <sup><a href="#fn_32" id="reffn_32">32</a></sup> ，基于射线铸造（ray casting）的采样 <sup><a href="#fn_13" id="reffn_13">13</a></sup> ，搜索整个边界 <sup><a href="#fn_16" id="reffn_16">16</a></sup> ，或从颜色簇（color clusters）采样 <sup><a href="#fn_28" id="reffn_28">28</a></sup><sup><a href="#fn_12" id="reffn_12">12</a></sup> 。在采样候选项中确定度量（metric）几乎总是包含一个抠图方程式重构误差（matting equation reconstruction error），可能由于测量样本与给定像素的距离 <sup><a href="#fn_32" id="reffn_32">32</a></sup><sup><a href="#fn_16" id="reffn_16">16</a></sup> 或者前景和背景样本的相似性 <sup><a href="#fn_32" id="reffn_32">32</a></sup><sup><a href="#fn_28" id="reffn_28">28</a></sup> ，以及包括稀疏编码 <sup><a href="#fn_12" id="reffn_12">12</a></sup> 和 KL-divergence （KL-发散）方法 <sup><a href="#fn_19" id="reffn_19">19</a></sup><sup><a href="#fn_18" id="reffn_18">18</a></sup> 。像纹理 <sup><a href="#fn_27" id="reffn_27">27</a></sup> 等高阶特征已经很少使用，并且效果有限。</p>
<p>在传播方法中，公式 <script type="math/tex">(1)</script> 重新配置，使其允许 <script type="math/tex">α</script> 值从已知的前景和背景区域传播（propagation）到未知区域。流行的方法是使用通常作为采样后的后处理 <sup><a href="#fn_32" id="reffn_32">32</a></sup><sup><a href="#fn_16" id="reffn_16">16</a></sup><sup><a href="#fn_28" id="reffn_28">28</a></sup> 的 Close-form 抠图 <sup><a href="#fn_22" id="reffn_22">22</a></sup> 。它从前景和背景颜色的局部平滑度假设推导出一个成本函数，并通过求解稀疏线性系统方程组找到全局最优的 <script type="math/tex">α</script> 蒙版（alpha matte）。其他的传播方法包括随机游走（random walks）<sup><a href="#fn_14" id="reffn_14">14</a></sup> ，求解泊松方程（Poisson equation）<sup><a href="#fn_31" id="reffn_31">31</a></sup> 和非本地传播方法（nolocal propagation methods）<sup><a href="#fn_21" id="reffn_21">21</a></sup><sup><a href="#fn_7" id="reffn_7">7</a></sup><sup><a href="#fn_5" id="reffn_5">5</a></sup> 。</p>
<p>近年来，人们提出了一些深度学习方法来解决抠图问题。然而，不能直接从给定图像和三分图中学习其 <script type="math/tex">α</script> 蒙版（alpha matte）。Shen 等人 <sup><a href="#fn_29" id="reffn_29">29</a></sup> 使用深度学习创建肖像图中人物的三分图，并使用 <sup><a href="#fn_22" id="reffn_22">22</a></sup> 进行抠图，将抠图误差（error）反向传播到网络中。Cho 等人<sup><a href="#fn_8" id="reffn_8">8</a></sup> 采用 <sup><a href="#fn_22" id="reffn_22">22</a></sup> 和 <sup><a href="#fn_5" id="reffn_5">5</a></sup> 的抠图结果，将 RGB 颜色归一化为输入，并训练一个端到端的深度网络来预测新的 <script type="math/tex">α</script> 蒙版（alpha matte）。虽然我们的算法和这两个项目都利用了深度学习，但我们的算法与它们的算法完全不同。我们的算法直接学习给定从给定图像和三分图直接学习对应的 <script type="math/tex">α</script> 蒙版（alpha matte）而另外两个项目依赖于现存的算法来计算实际的抠图问题，使得他们的方法容易出现和以前的抠图方法同样的问题。</p>
<p><img src="https://miao.su/images/2019/03/13/1552483942593f2efa.png" alt></p>
<blockquote>
<p>图 <a href="#2">2</a>. 数据集的创建。 (a) 包含简单背景的手动抠图的输入图像， (b) 计算 alpha 蒙版和 (c) 计算的前景色用作数据真值用来将对象合成到 (d-f) 各种背景图上。</p>
</blockquote>
<h2 id="3-新的抠图数据集"><a href="#3-新的抠图数据集" class="headerlink" title="3. 新的抠图数据集"></a>3. 新的抠图数据集</h2><p><a href="http://alphamatting.com/" target="_blank" rel="noopener">alphamatting.com</a> <sup><a href="#fn_25" id="reffn_25">25</a></sup> 上的抠图基准在推动抠图研究的进步方面取得了巨大的成功。然而，由于获取数据真值（ground truth）图像需要细心且繁琐的工作，此数据集仅由 27 张训练图像和 8 张测试图像构成。这不仅不足以训练神经网络，而且其多样性受到严重限制，仅限于具有静态物体的小规模实验室场景。</p>
<p>为了训练我们的抠图网络，我们通过将真实图像中的对象合成到新的背景来创建更大的数据集。我们在简单的背景上找到图像（图 <a href="#2">2</a>.a），包括来自 <sup><a href="#fn_25" id="reffn_25">25</a></sup> 的 27 个训练图像和来自 <sup><a href="#fn_26" id="reffn_26">26</a></sup> 的视频的每五个帧。使用 PhotoShop 我们手动创建了 <script type="math/tex">α</script> 蒙版（alpha matte）（图 <a href="#2">2</a>. b）和纯前景图像（图 <a href="#2">2</a>. c）。因为这些对象具有简单的背景，所以我们可以为他们提供准确的蒙版。然后我们将这些作为基本的标准结果（ground truth），对于每个 <script type="math/tex">α</script> 蒙版和前景图像，我们在 MS COCO <sup><a href="#fn_23" id="reffn_23">23</a></sup> 和 Pascal VOC <sup><a href="#fn_11" id="reffn_11">11</a></sup> 中随机抽取 N 个背景图像，并将对象合成到这些背景图像上。</p>
<p>我们使用上述方法创建了训练数据集和测试数据集。我们的训练数据集拥有 493 个独特的前景对象和 49300 个图像（N = 100），而我们的测试数据集有 50 个独立对象和 1000 个图像（N = 20）。每个图像的三分图从其真实的 <script type="math/tex">α</script> 蒙版中随机膨胀（dilate）而成。与之前的抠图数据集相比，我们的新数据集有以下几个优点：1) 它拥有更多的独特的对象，并涵盖各种抠图的情况，如毛发、毛皮、半透明度等。2) 许多合成图像具有相似的前景色和背景色以及复杂的背景纹理，使我们的数据集更具有挑战性和使用性。</p>
<p>一个早期的问题是，由于图像的合成性质，这个过程是否会产生偏差，因此网络应学会关注前景和背景亮度、噪声水平等的差异。然而，我们通过实验发现，与现有方法相比，我们在自然图像上获得了更好的结果（见 <a href="#5.3">5.3</a> 节）。</p>
<h2 id="4-我们的方法"><a href="#4-我们的方法" class="headerlink" title="4. 我们的方法"></a>4. 我们的方法</h2><p>我们使用深度学习来解决抠图问题。鉴于我们的新数据集，我们训练一个神经网络以充分利用数据。该网络有两个阶段组成（图 <a href="#3">3</a>）。第一阶段是深度卷积编码-解码器网络，它采用图像小块（image patch）和三分图作为输入，并且因为 <script type="math/tex">α</script> 预测损失和新的（组合损失compositional loss）而有不利之处。第二阶段是一个小的全卷积网络，它以更准确的 alpha 值和更清晰的边缘完善了第一个网络的 alpha 预测。我们将在以下各节中详细描述我们的算法。</p>
<p><img src="https://miao.su/images/2019/03/13/1552372427134b4b5c.png" alt></p>
<blockquote>
<p>图 <a href="#3">3</a>. 我们的网络包含两个阶段，一个是编码器-解码器阶段（<a href="#4.1">4.1</a>节），一个是精炼阶段（<a href="#4.2">4.2</a>节）</p>
</blockquote>
<h3 id="4-1-抠图编码器-解码器阶段（Matting-encoder-decoder-stage）"><a href="#4-1-抠图编码器-解码器阶段（Matting-encoder-decoder-stage）" class="headerlink" title="4.1. 抠图编码器-解码器阶段（Matting encoder-decoder stage）"></a><a href="#4.1">4.1</a>. 抠图编码器-解码器阶段（Matting encoder-decoder stage）</h3><p>我们的第一阶段是一个深度编码-解码器网络（参见图 <a href="#3">3</a>.），它在许多其他计算机视觉任务中取得了成功，例如图像分割 <sup><a href="#fn_2" id="reffn_2">2</a></sup>，边缘预测<sup><a href="#fn_33" id="reffn_33">33</a></sup>和孔填充（hole filling）<sup><a href="#fn_24" id="reffn_24">24</a></sup>。</p>
<p><strong>网络结构（Network structure）：</strong></p>
<p>网络的输入时一个图像小块和相应的三分图，它们沿着通道维度连接，产生 4 通道输入。整个网络由编码器网络和解码器网络组成。通过后续卷积层和最大池化层将编码器网络的输入转化为下采样特征图（feature map）。解码器网络一次使用后续的反池化层来反转最大池化操作和卷积层来对特征映射进行上采样，并具有所需的输出，在我们的例子中是 alpha 蒙版。具体来说，我们的编码器网络有 14 个卷积层和 5 个最大池化层。对于解码器网络，我们使用比编码器网络更小的结构来减少参数数量并加快训练过程。具体来说，我们的解码器网络有 6 个卷积层，5 个反卷积层，后面是最终的 alpha 预测层。</p>
<p><strong>损失（Losses）：</strong></p>
<p>我们的网络利用了两次损失。第一个损失称为 <strong>alpha 预测损失（alpha-prediction loss）</strong>，即数据真值 alpha 值与每个像素的预测 alpha 值之间的绝对差值。但是，由于绝对值的不可微分性，我们使用以下损失函数来近似它。</p>
<script type="math/tex; mode=display">
\mathcal{L}_α^i = \sqrt{(α_p^i-α_g^i)^2 + \epsilon^2} ，\ \ \ \ α_p^i,α_g^i \in [0,1] \tag{2}</script><p>其中 <script type="math/tex">α_p^i</script> 是预测层在阈值化为 0 到 1 之间的像素 <script type="math/tex">i</script> 处的输出。<script type="math/tex">\alpha_g^i</script> 是像素 <script type="math/tex">i</script> 数据真值的 alpha 值。<script type="math/tex">\epsilon</script> 是一个很小的值，在我们的实验中等于 <script type="math/tex">10^{-6}</script> 。导数 <script type="math/tex">\frac{\partial \mathcal{L}\_\alpha^i}{\partial \alpha\_p^i}</script> 如下。</p>
<script type="math/tex; mode=display">
\frac{\partial \mathcal{L}_\alpha^i}{\partial \alpha_p^i} = \frac{α_p^i-α_g^i}{\sqrt{(α_p^i-α_g^i)^2 + \epsilon^2}} \tag{3}</script><p>第二个损失被称为<strong>合成损失（compositional loss）</strong>，它是数据真值的 RGB 颜色和由数据真值前景、背景和合成的 alpha 蒙版预测的 RGB 颜色之间的绝对差值。同样，我们可以使用以下损失函数来近似它。</p>
<script type="math/tex; mode=display">
\mathcal{L}_c^i = \sqrt{(c_p^i - c_g^i)^2 + \epsilon^2} \tag{4}</script><p>其中 <script type="math/tex">c</script> 表示 RGB 通道，<script type="math/tex">p</script> 表示由预测的 alpha 合成的图像，<script type="math/tex">g</script> 表示由数据真值 alpha 构成的图像。组合损失（compositional loss）限制网络遵循组合操作（compositional operation），使得 alpha 预测更加准确。</p>
<p>总损失是两个损失的加权求和，即 <script type="math/tex">\mathcal{L}_overall = {w_l} \cdot {\mathcal{L}_\alpha} + (1 - w_l) \cdot {\mathcal{L}_c}</script> ，其中 <script type="math/tex">w_l</script> 在我们的实验中设置为 <script type="math/tex">0.5</script> 。此外，因为只需要推断三分图未知区域的 alpha 值，所以我们根据像素位置对两种类型的损失设置额外的权重，这可以帮助我们的网络更注重于重要区域。具体地，如果像素 <script type="math/tex">i</script> 在三分图的未知区域内，则 <script type="math/tex">w_i = 1</script> ，否则 <script type="math/tex">w_i = 0</script> 。</p>
<p><strong>实现（Implementation）：</strong></p>
<p>虽然我们的训练数据集有 49300 个图像，但只有 493 个唯一对象。为了避免过拟合以及更有效的利用训练数据，我们使用了一些训练策略。首先，我们以未知区域中的像素为中心随机裁剪为 <script type="math/tex">320\times320 \ (image, trimap)</script> 。这增加了我们的采样空间。其次，我们还裁剪了不同尺寸的训练对（例如：<script type="math/tex">480\times480 , 640\times640</script>）并将其调整为 <script type="math/tex">320\times320</script> 。这使得我们的方法在扩展方面更加健壮，并有助于网络更好的学习上下文和语义。第三，在每个训练对上随机进行翻转（flipping）。第四，三分图从它们的数据真值 alpha 蒙版（ground truth alpha mattes）中随机扩张，帮助我们的模型对三分图的放置（placement）更稳健。最后，在每个训练周期结束后随机重建训练输入。</p>
<p>网络的编码器部分是用 VGG-16 <sup><a href="#fn_30" id="reffn_30">30</a></sup> 的前 14 个卷积层初始化的（第 14 层是全连接层 “fc6” ，可以转换为卷积层）由于网络具有 4 通道输入，我们用全零初始化第一层卷积滤波器的一个额外通道。所有解码器参数都用 Xavier 随机初始化。</p>
<p>测试时，图像和相应的三分图作为输入串联。执行网络的前向传播以输出 alpha 蒙版预测。当 GPU 内存不足以容纳大图像时，可以执行 CPU 测试。</p>
<h3 id="4-2-抠图细化阶段（Matting-refinement-stage）："><a href="#4-2-抠图细化阶段（Matting-refinement-stage）：" class="headerlink" title="4.2. 抠图细化阶段（Matting refinement stage）："></a><a href="#4.2">4.2</a>. 抠图细化阶段（Matting refinement stage）：</h3><p>尽管来自我们网络第一部分的 alpha 预测已经比现有的抠图算法好得多，但由于编码器-解码器结构，结果有时过于平滑。因此，我们扩展我们的网络以进一步完善第一部分的结果。这种扩展的网络通常可以预测更准确的 alpha 蒙版和更清晰的边缘。</p>
<p><strong>网络结构（Network structure）：</strong></p>
<p>我们网络的第二阶段的输入是图像小块（image patch）与第一阶段的 alpha 预测（在 0 到 255 之间缩放）的串联，从而产生四通道输入。输出是相应的相应的数据真值（ground truth）的 alpha 蒙版。该网络是一个完全卷积网络，包括 4 个卷积层。前三个卷积层中的每个后面是非线性 ReLU 层。没有下采样层，因为我们希望保留第一阶段中非常细微的结构。此外，我们使用了 ”skip-model“ 结构，其中输入数据的第 4 个通道首先在 0 到 1 之间缩放（scaled），然后在网络的输出上添加。详细配置如图 3 所示。</p>
<p>我们的细化阶段的效果如图 4 所示。注意，它不会对 alpha 蒙版进行大规模更改，而只是细化和锐化 alpha 值。</p>
<p><img src="https://miao.su/images/2019/03/13/155237991322231706.png" alt></p>
<blockquote>
<p>图 <a href="#4">4</a>. 我们抠图细化网络的影响。(a) 输入图像。(b) 编码器-解码器阶段结果。(c) 抠图细化层结果。</p>
</blockquote>
<p><strong>实现（Implementation）：</strong></p>
<p>在训练期间，我们首先更新编码器-解码器部分而不使用细化部分。在编码器-解码器部分收敛之后，我们修复其参数，然后更新细化部分。由于其结构简单，仅使用 alpha 预测损失（公式 <script type="math/tex">2</script>）。除第 4 阶段外，我们还使用了第 1 阶段的所有训练策略。在细化部分也收敛之后，最后我们一起微调整个网络。我们使用 Adam <sup><a href="#fn_20" id="reffn_20">20</a></sup> 来更新这两个部分。较小的学习率 <script type="math/tex">10^{-5}</script>​ 在训练过程中不断被设置。</p>
<p>测试时，输入图像和对应的三分图，我们的算法首先使用抠图网络的编码器-解码器阶段来获得初始的 alpha 蒙版预测。然后将图像和 alpha 蒙版预测连接作为细化阶段的输入以产生最终的 alpha 蒙版预测。</p>
<h2 id="5-实验结果"><a href="#5-实验结果" class="headerlink" title="5. 实验结果"></a>5. 实验结果</h2><p>在本节，我们将在 3 个数据集上评估我们的方法。1）在抠图方法的现有基准 <a href="http://alphamatting.com/" target="_blank" rel="noopener">alphamatting.com</a> 数据集 <sup><a href="#fn_25" id="reffn_25">25</a></sup> 上评估。它包括 8 个测试图像，每个图像有 3 个不同的三分图，即 “small” 、”large” 和 “user” 。2）由于 <a href="http://alphamatting.com/" target="_blank" rel="noopener">alphamatting.com</a> 数据集中对象的大小和范围有限，我们建议使用 Composition-1k 测试集。我们基于 Composition 的数据集包含 1000 张图像和 50 个独特的前景。此数据集具有更广泛的对象类型和背景场景。3）为了测试我们的方法在自然图像上的表现，我们还收集了包括 31 个自然图像的第三个数据集。自然图像涵盖了广泛的常见抠图前景，比如人、动物等。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Methods</th>
<th>SAD</th>
<th>MSE</th>
<th>Gradient</th>
<th>Connectivity</th>
</tr>
</thead>
<tbody>
<tr>
<td>Shared Matting <sup><a href="#fn_13" id="reffn_13">13</a></sup></td>
<td>128.9</td>
<td>0.091</td>
<td>126.5</td>
<td>135.3</td>
</tr>
<tr>
<td>Learning Based Matting <sup><a href="#fn_34" id="reffn_34">34</a></sup></td>
<td>113.9</td>
<td>0.048</td>
<td>91.6</td>
<td>122.2</td>
</tr>
<tr>
<td>Comprehensive Sampling <sup><a href="#fn_28" id="reffn_28">28</a></sup></td>
<td>143.8</td>
<td>0.071</td>
<td>102.2</td>
<td>142.7</td>
</tr>
<tr>
<td>Global Matting <sup><a href="#fn_16" id="reffn_16">16</a></sup></td>
<td>133.6</td>
<td>0.068</td>
<td>97.6</td>
<td>133.3</td>
</tr>
<tr>
<td>Closed-Form Matting <sup><a href="#fn_22" id="reffn_22">22</a></sup></td>
<td>168.1</td>
<td>0.091</td>
<td>126.9</td>
<td>167.9</td>
</tr>
<tr>
<td>KNN Matting <sup><a href="#fn_5" id="reffn_5">5</a></sup></td>
<td>175.4</td>
<td>0.103</td>
<td>124.1</td>
<td>176.4</td>
</tr>
<tr>
<td>DCNN Matting <sup><a href="#fn_8" id="reffn_8">8</a></sup></td>
<td>161.4</td>
<td>0.087</td>
<td>115.1</td>
<td>161.9</td>
</tr>
<tr>
<td>——————</td>
<td>——-</td>
<td>——-</td>
<td>————</td>
<td>——————</td>
</tr>
<tr>
<td><em>Encoder-Decoder   network</em>   <em>(single   alpha prediction loss)</em></td>
<td>59.6</td>
<td>0.019</td>
<td>40.5</td>
<td>59.3</td>
</tr>
<tr>
<td><em>Encoder-Decoder   network</em></td>
<td>54.6</td>
<td>0.017</td>
<td>36.7</td>
<td>55.3</td>
</tr>
<tr>
<td><em>Encoder-Decoder   network + Guided filter</em><sup><a href="#fn_17" id="reffn_17">17</a></sup></td>
<td>52.2</td>
<td>0.016</td>
<td><strong>30.0</strong></td>
<td>52.6</td>
</tr>
<tr>
<td><em>Encoder-Decoder   network + Refinement network</em></td>
<td><strong>50.4</strong></td>
<td><strong>0.014</strong></td>
<td>31.0</td>
<td><strong>50.8</strong></td>
</tr>
</tbody>
</table>
</div>
<blockquote>
<p><a href="#表1">表 1</a>. Composition-1k 测试数据集的定量结果。我么的方法的变体以斜体强调。最佳的结果以粗体强调。</p>
</blockquote>
<h3 id="5-1-alphamatting-com-数据集"><a href="#5-1-alphamatting-com-数据集" class="headerlink" title="5.1. alphamatting.com 数据集"></a>5.1. <a href="http://alphamatting.com/" target="_blank" rel="noopener">alphamatting.com</a> 数据集</h3><p>与 <a href="http://alphamatting.com/" target="_blank" rel="noopener">alphamatting.com</a> 基准测试中的所有其他方法相比，我们的方法取得了最佳结果。具体来说，我们的方法在 SAD 指标方面排名第一。对于具有 3 个三分图的 5 个图像（图 <a href="#5">5</a>.），我们的方法也具有最小的 SAD 误差。此外，我们的方法在 MSE 和 Gradient 指标方面排名第二。总的来说，我们的方法是该数据集中表现最佳的方法之一。</p>
<p>我们成功的一个关键原因是我们的网络学习结构和语义的能力，这对于在背景场景复杂或背景和前景色相似时准确估计 alpha 蒙版非常重要。例如，在图 <a href="#6">6</a> 中，”Troll” 实例具有非常相似的头发和桥的颜色，而 “Doll” 实例具有强烈的纹理背景。以前方法的最佳结果（从第 3 列到第 6 列）在这些硬区域都有明显的错误。相比之下，我们的方法直接学习对象结构和图像上下文。因此，我们的方法不仅避免了以前方法所犯的类似错误，而且还预测了更过细节。值得注意的时，虽然 DCNN matting <sup><a href="#fn_8" id="reffn_8">8</a></sup> 也是一种基于深度学习的方法，它学习了小型局部先前具有小型局部补丁的抠图方法的分线性组合（it learns the non-linear combination of previous matting methods within small local patches）。因此，该方法不能真正理解语义，因此具有与先前的非基于深度学习的方法相同的限制。</p>
<p><img src="https://miao.su/images/2019/03/13/1552392001716977c3.png" alt></p>
<blockquote>
<p>图 <a href="#5">5</a>. alphamatting.com 数据集上的 SAD 结果。展示了前五个方法。我们的方法用红框强调。</p>
</blockquote>
<p><img src="https://miao.su/images/2019/03/13/155239233423197481.png" alt="Troll"><img src="https://miao.su/images/2019/03/13/1552392382192d4998.png" alt="Ours"><img src="https://miao.su/images/2019/03/13/155239239608952146.png" alt="Local-Sampling"><img src="https://miao.su/images/2019/03/13/1552392408598fb05f.png" alt="TSPS-RV"><img src="https://miao.su/images/2019/03/13/15523924257746ddd9.png" alt="CSC"><img src="https://miao.su/images/2019/03/13/1552392440095e5bf4.png" alt="DCNN"></p>
<p><img src="https://miao.su/images/2019/03/13/1552392578792b84c6.png" alt="Doll"><img src="https://miao.su/images/2019/03/13/1552392592015c7238.png" alt="Ours"><img src="https://miao.su/images/2019/03/13/15523926030398e34a.png" alt="DCNN"><img src="https://miao.su/images/2019/03/13/15523926142868c7ce.png" alt="LNSP"><img src="https://miao.su/images/2019/03/13/15523926259836c4b0.png" alt="KL-Divergence"><img src="https://miao.su/images/2019/03/13/155239263790389b01.png" alt="Iterative"></p>
<blockquote>
<p>图 <a href="#6">6</a>. 测试图像的alpha蒙版预测，其中”Troll“三分图为”user“，”Doll“三分图为”small“。第一列显示测试图像。对于每个测试图像，从第２列到第６列显示了在SAD度量下的第一名到第五名。在两个例子中，我们的方法都时最佳结果。</p>
<p>Local-Sampling<sup><a href="#fn_6" id="reffn_6">6</a></sup> TSPS-RV<sup><a href="#fn_1" id="reffn_1">1</a></sup> CSC<sup><a href="#fn_12" id="reffn_12">12</a></sup> DCNN<sup><a href="#fn_8" id="reffn_8">8</a></sup> LNSP<sup><a href="#fn_7" id="reffn_7">7</a></sup> KL-Divergence<sup><a href="#fn_19" id="reffn_19">19</a></sup> Iterative<sup><a href="#fn_15" id="reffn_15">15</a></sup></p>
</blockquote>
<h3 id="5-2-Composition-1k-测试数据集"><a href="#5-2-Composition-1k-测试数据集" class="headerlink" title="5.2. Composition-1k 测试数据集"></a>5.2. Composition-1k 测试数据集</h3><p>我们在 Composition-1k 测试数据集上进一步评估了 7 种先前表现最佳的方法和我们方法的每个组成部分。对于先前的方法，使用作者提供的代码。我们方法的不同变体包括：抠图编码器-解码器网络 1）只有 alpha 预测损失，2）包含 alpha 预测损失和组合损失（compositional loss），抠图编码器-解码器网络 3）由 Guided filter<sup><a href="#fn_17" id="reffn_17">17</a></sup> 后处理（post-processed）和 4）抠图细化网络（matting refinement network）后处理。</p>
<p>[25] 中提出的 SAD，MSE，Gradient 和 Connectivity 误差下的定量结果显示在表 1 中。显然我们方法的所有变体都比其他方法有更好的结果。主要原因仍然是我们的深度模型能够理解图像的复杂背景，而其他方法则无法理解。通过比较我们方法的变体，我们还可以验证我们方法的每个组成部分的有效性：1）组合损失（compositional loss）有助于我们的模型学习组合操作（compositional operation），从而产生更好的结果，2）通过与先前的 edge-preserving filter（例如：Guided filter<sup><a href="#fn_17" id="reffn_17">17</a></sup>）以及我们的抠图细化网络相结合，可以改善我们的抠图编码器-解码器网络的结果。但是，后者在视觉上和数量上都有更明显的改进，因为它直接用我们的编码器-解码器网络的输出进行训练。</p>
<p>我们在图 7 中测试了我们的方法对三分图放置（placement）的灵敏度。我们对数据集的一个子集进行评估，其中包括每个唯一对象的一个随机选择的图像，总共 50 个图像。为了形成三分图，我们将每个图像的数据真值（ground truth）alpha扩展 <script type="math/tex">d</script> 个像素以增加 <script type="math/tex">d</script> 的值。特定参数 <script type="math/tex">d</script> 处的 SAD 误差是在所有图像上的平均。参数 <script type="math/tex">d\in[1,4,7,10,13,16,19]</script> 的所有方法的结果如图 7 所示。显然，我们的方法具有低且稳定的错误率，随着 <script type="math/tex">d</script> 值的增加，而其他方法的错误率迅速增加。我们的良好表现源于我们的训练策略以及对图像语义的良好理解。</p>
<p><img src="https://miao.su/images/2019/03/13/15523994212486ce95.png" alt></p>
<blockquote>
<p>图 <a href="#7">7</a>. 不同水平三分图扩张的 SAD 误差。</p>
</blockquote>
<p>图 <a href="#8">8</a> 显示了一些可见的例子，表明我们的方法在不同的抠图情况下的良好表现，如头发、洞和半透明。此外，我们的方法还可以处理没有纯前景像素的对象，如图 <a href="#8">8</a> 中的最后一个实例所示。由于先前基于采样和基于传播的方法必须利用已知的前景和背景像素，因此它们无法处理这种情况，而我们的方法可以直接从数据中学习精细细节的外观。</p>
<p><img src="https://miao.su/images/2019/03/13/15524000053679b0b5.png" alt></p>
<blockquote>
<p>图 <a href="#8">8</a>. Compositional-1k 测试数据集的视觉比较结果。“Ours-raw”表明了我们的抠图编码器-解码器阶段的结果，而“Ours-refined”表明了我们的精细化阶段的结果。</p>
</blockquote>
<h3 id="5-3-真实图像数据集"><a href="#5-3-真实图像数据集" class="headerlink" title="5.3. 真实图像数据集"></a><a href="#5.3">5.3</a>. 真实图像数据集</h3><p>抠图方法应该能够很好的扩展到真实世界的图像。为了验证我们的方法和其他方法在真实图像上的性能我们对真实图像数据集进行了用户研究。这些图像包括从互联网上提取的图像以及 ICCV 2013 教程中关于抠图的图像。</p>
<p>因为我们的受试者可能不熟悉 alpha 蒙版，所以我们改为评估组合物的结果。对于每种方法，计算的 alpha 蒙版用于混合测试图像到黑色背景上和白色背景上。对于用户测试，我们向用户呈现图像和两个随机选择的方法的两个合成结果，并询问那些结果更准确和更真实，尤其是在精细细节的区域（例如头发、对象的边缘和半透明区域）。为了避免评估误差，我们在 Amazon Mechanical Turk 上进行用户研究。结果，共有 392 个用户参与用户研究，并且一个图像上的每个方法对由 5 到 6 个唯一用户评估。</p>
<p>成对比较结果显示在表 2 中，其中，每列显示一种优于其他方法的方法。例如，用户 83.7% 的时间更喜欢我们的结果对于 <sup><a href="#fn_13" id="reffn_13">13</a></sup> 。值得注意的是，近五分之四的用户更喜欢我们的方法超过现有的方法，这很好地证明了我们的方法确实产生了更好的视觉效果。有关视觉效果，参见图 <a href="#9">9</a> 。</p>
<p><img src="https://miao.su/images/2019/03/13/15524023237505c1e7.png" alt></p>
<blockquote>
<p>图 <a href="#9">9</a>. 我们的真实图像数据集的示例结果。</p>
</blockquote>
<div class="table-container">
<table>
<thead>
<tr>
<th>Methods</th>
<th>[13]</th>
<th>[34]</th>
<th>[28]</th>
<th>[16]</th>
<th>[22]</th>
<th>[5]</th>
<th>[8]</th>
<th>Ours</th>
</tr>
</thead>
<tbody>
<tr>
<td>Shared <sup><a href="#fn_13" id="reffn_13">13</a></sup></td>
<td>-</td>
<td>60.0</td>
<td><strong>78.5</strong></td>
<td><strong>79.6</strong></td>
<td>69.7</td>
<td>40.6</td>
<td><strong>57.8</strong></td>
<td><strong>83.7</strong></td>
</tr>
<tr>
<td>Learning <sup><a href="#fn_34" id="reffn_34">34</a></sup></td>
<td>40.0</td>
<td>-</td>
<td><strong>60.2</strong></td>
<td><strong>54.6</strong></td>
<td><strong>53.4</strong></td>
<td>27.3</td>
<td>35.1</td>
<td><strong>83.6</strong></td>
</tr>
<tr>
<td>Comprehensive <sup><a href="#fn_28" id="reffn_28">28</a></sup></td>
<td>21.5</td>
<td>39.8</td>
<td>-</td>
<td>25.8</td>
<td>43.3</td>
<td>20.4</td>
<td>29.2</td>
<td><strong>78.8</strong></td>
</tr>
<tr>
<td>Global <sup><a href="#fn_16" id="reffn_16">16</a></sup></td>
<td>20.4</td>
<td>45.4</td>
<td><strong>74.2</strong></td>
<td>-</td>
<td><strong>53.3</strong></td>
<td>30.0</td>
<td>42.0</td>
<td><strong>84.2</strong></td>
</tr>
<tr>
<td>Closed-Form<sup><a href="#fn_22" id="reffn_22">22</a></sup></td>
<td>30.3</td>
<td>46.6</td>
<td><strong>56.7</strong></td>
<td>46.7</td>
<td>-</td>
<td>25.0</td>
<td>38.1</td>
<td><strong>80.4</strong></td>
</tr>
<tr>
<td>KNN<sup><a href="#fn_5" id="reffn_5">5</a></sup></td>
<td><strong>59.4</strong></td>
<td><strong>72.7</strong></td>
<td><strong>79.6</strong></td>
<td>70.0</td>
<td><strong>75.0</strong></td>
<td>-</td>
<td><strong>73.3</strong></td>
<td><strong>97.0</strong></td>
</tr>
<tr>
<td>DCNN <sup><a href="#fn_8" id="reffn_8">8</a></sup></td>
<td>42.2</td>
<td><strong>64.9</strong></td>
<td>70.8</td>
<td><strong>58.0</strong></td>
<td><strong>61.9</strong></td>
<td>26.7</td>
<td>-</td>
<td><strong>83.7</strong></td>
</tr>
<tr>
<td>Ours</td>
<td>16.3</td>
<td>16.4</td>
<td>21.2</td>
<td>15.8</td>
<td>19.6</td>
<td>3.0</td>
<td>16.3</td>
<td>-</td>
</tr>
</tbody>
</table>
</div>
<p>同样值得注意的是，与其他两种实验相比，本实验中的其他方法的排名不同。例如，Closed-Form Matting <sup><a href="#fn_22" id="reffn_22">22</a></sup> 是我们在 <a href="http://alphamatting.com/" target="_blank" rel="noopener">alphamatting.com</a> 数据集上比较的方法中排名最低的，但对用户来说，它比除我们和 <sup><a href="#fn_28" id="reffn_28">28</a></sup> 的其他所有方法更可取。另一方面，尽管 DCNN <sup><a href="#fn_8" id="reffn_8">8</a></sup> 是先前在 <a href="http://alphamatting.com/" target="_blank" rel="noopener">alphamatting.com</a> 上最先进的方法，但在真实图像上仅优于两种方法。目前还不清楚这究竟是由于方法过度拟合了 <a href="http://alphamatting.com/" target="_blank" rel="noopener">alphamatting.com</a> 数据集，还是标准错误指标未能准确衡量人类对 alpha 蒙版结果的感知判断。</p>
<h2 id="6-结论"><a href="#6-结论" class="headerlink" title="6. 结论"></a>6. 结论</h2><p>为了推广到自然图像，抠图算法必须避免使用颜色作为抠图的主要依据，并利用更多的结构和语义特征。在这项工作中，我们展示了神经网络能够捕获这样的高阶特征并应用它们来计算更好的抠图结果。我们的实验表明，我们的方法不仅优于标准数据集上的先前的方法，而且能够更好的推广到真实图像。</p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><blockquote id="fn_1">
<sup>1</sup>. A. Al-Kabbany and E. Dubois. Matting with sequential pair selection using graph transduction. In <em>21st International Symposium on Vision, Modeling, and Visualization</em>, 2016. 6<a href="#reffn_1" title="Jump back to footnote [1] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_2">
<sup>2</sup>. V. Badrinarayanan, A. Handa, and R. Cipolla. Segnet: A deep convolutional encoder-decoder architecture for robust semantic pixel-wise labelling. <em>arXiv preprint arXiv:1505.07293</em>, 2015. 3<a href="#reffn_2" title="Jump back to footnote [2] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_3">
<sup>3</sup>. A. Berman, A. Dadourian, and P. Vlahos. Method for removing from an image the background surrounding a selected object, Oct. 17 2000. US Patent 6,134,346. 1, 2<a href="#reffn_3" title="Jump back to footnote [3] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_4">
<sup>4</sup>. D. J. Butler, J. Wulff, G. B. Stanley, and M. J. Black. A naturalistic open source movie for optical flow evaluation. In <em>Proceedings of the European Conference on Computer Vision</em>, 2012. 2<a href="#reffn_4" title="Jump back to footnote [4] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_5">
<sup>5</sup>. Q. Chen, D. Li, and C.-K. Tang. Knn matting. <em>IEEE transactions on pattern analysis and machine intelligence</em>, 35(9):2175–2188, 2013. 2, 5, 7, 8<a href="#reffn_5" title="Jump back to footnote [5] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_6">
<sup>6</sup>. X. Chen and F. He. A propagation matting method based on the local sampling and knn classification with adaptive feature space. <em>Journal of Computer-Aided Design and Computer Graphics</em>, 2016. 6<a href="#reffn_6" title="Jump back to footnote [6] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_7">
<sup>7</sup>. X. Chen, D. Zou, S. Zhiying Zhou, Q. Zhao, and P. Tan. Image matting with local and nonlocal smooth priors. In <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em>, pages 1902–1907, 2013. 2, 6<a href="#reffn_7" title="Jump back to footnote [7] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_8">
<sup>8</sup>. D. Cho, Y.-W. Tai, I. Kweon, D. Cho, Y.-W. Tai, and I. Kweon. Natural image matting using deep convolutional neural networks. In <em>Proceedings of the European Conference on Computer Vision</em>, 2016. 1, 2, 5, 6, 7, 8<a href="#reffn_8" title="Jump back to footnote [8] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_9">
<sup>9</sup>. Y.-Y. Chuang, B. Curless, D. H. Salesin, and R. Szeliski. A bayesian approach to digital matting. In <em>Computer Vision and Pattern Recognition, 2001. CVPR 2001. Proceedings of the 2001 IEEE Computer Society Conference on</em>, volume 2, pages II–264. IEEE, 2001. 1, 2<a href="#reffn_9" title="Jump back to footnote [9] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_10">
<sup>10</sup>. M. Erofeev, Y. Gitman, D. Vatolin, A. Fedorov, and J. Wang. Perceptually motivated benchmark for video matting. In <em>Proceedings of the British Machine Vision Conference (BMVC)</em>, 2015. 1<a href="#reffn_10" title="Jump back to footnote [10] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_11">
<sup>11</sup>. M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and A. Zisserman. The pascal visual object classes (voc) challenge. <em>International journal of computer vision</em>, 88(2):303– 338, 2010. 3<a href="#reffn_11" title="Jump back to footnote [11] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_12">
<sup>12</sup>. X. Feng, X. Liang, and Z. Zhang. A cluster sampling method for image matting via sparse coding. In <em>European Conference on Computer Vision</em>, pages 204–219. Springer, 2016. 2, 6<a href="#reffn_12" title="Jump back to footnote [12] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_13">
<sup>13</sup>. E. S. Gastal and M. M. Oliveira. Shared sampling for realtime alpha matting. In <em>Computer Graphics Forum</em>, volume 29, pages 575–584. Wiley Online Library, 2010. 1, 2, 5, 7, 8<a href="#reffn_13" title="Jump back to footnote [13] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_14">
<sup>14</sup>. L. Grady, T. Schiwietz, S. Aharon, and R. Westermann. Random walks for interactive alpha-matting. In <em>Proceedings of VIIP</em>, volume 2005, pages 423–429, 2005. 1, 2<a href="#reffn_14" title="Jump back to footnote [14] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_15">
<sup>15</sup>. B. He, G. Wang, C. Shi, X. Yin, B. Liu, and X. Lin. Iterative transductive learning for alpha matting. In <em>2013 IEEE International Conference on Image Processing</em>, pages 4282– 4286. IEEE, 2013. 6<a href="#reffn_15" title="Jump back to footnote [15] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_16">
<sup>16</sup>. K. He, C. Rhemann, C. Rother, X. Tang, and J. Sun. A global sampling method for alpha matting. In <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em>, 2011. 1, 2, 5, 7, 8<a href="#reffn_16" title="Jump back to footnote [16] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_17">
<sup>17</sup>. K. He, J. Sun, and X. Tang. Guided image filtering. In <em>European conference on computer vision</em>, pages 1–14. Springer, 2010. 5, 6<a href="#reffn_17" title="Jump back to footnote [17] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_18">
<sup>18</sup>. J. Johnson, E. S. Varnousfaderani, H. Cholakkal, , and D. Rajan. Sparse coding for alpha matting. <em>IEEE Transactions on Image Processing</em>, 2016. 2<a href="#reffn_18" title="Jump back to footnote [18] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_19">
<sup>19</sup>. L. Karacan, A. Erdem, and E. Erdem. Image matting with kl-divergence based sparse sampling. In <em>Proceedings of the IEEE International Conference on Computer Vision</em>, pages 424–432, 2015. 2, 6<a href="#reffn_19" title="Jump back to footnote [19] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_20">
<sup>20</sup>. D. Kingma and J. Ba. Adam: A method for stochastic optimization. <em>International Conference on Learning Representations</em>, 2015. 5<a href="#reffn_20" title="Jump back to footnote [20] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_21">
<sup>21</sup>. P. Lee and Y. Wu. Nonlocal matting. <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em>, 2011. 2<a href="#reffn_21" title="Jump back to footnote [21] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_22">
<sup>22</sup>. A. Levin, D. Lischinski, and Y. Weiss. A closed-form solution to natural image matting. <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, 30(2):228–242, 2008. 1, 2, 5, 7, 8<a href="#reffn_22" title="Jump back to footnote [22] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_23">
<sup>23</sup>. T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollar, and C. L. Zitnick. Microsoft coco: Com-´ mon objects in context. In <em>European Conference on Computer Vision</em>, pages 740–755. Springer, 2014. 3<a href="#reffn_23" title="Jump back to footnote [23] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_24">
<sup>24</sup>. D. Pathak, P. Krahenbuhl, J. Donahue, T. Darrell, and A. A. Efros. Context encoders: Feature learning by inpainting. In <em>The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, June 2016. 3<a href="#reffn_24" title="Jump back to footnote [24] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_25">
<sup>25</sup>. C. Rhemann, C. Rother, J. Wang, M. Gelautz, P. Kohli, and P. Rott. A perceptually motivated online benchmark for image matting. In <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em>, 2009. 1, 2, 3, 5, 6<a href="#reffn_25" title="Jump back to footnote [25] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_26">
<sup>26</sup>. E. Shahrian, B. Price, S. Cohen, and D. Rajan. Temporally coherent and spatially accurate video matting. In <em>Proceedings of Eurographics</em>, 2012. 3<a href="#reffn_26" title="Jump back to footnote [26] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_27">
<sup>27</sup>. E. Shahrian and D. Rajan. Weighted color and texture sample selection for image matting. In <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em>, 2012. 2<a href="#reffn_27" title="Jump back to footnote [27] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_28">
<sup>28</sup>. E. Shahrian, D. Rajan, B. Price, and S. Cohen. Improving image matting using comprehensive sampling sets. In <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em>, pages 636–643, 2013. 1, 2, 5, 7, 8<a href="#reffn_28" title="Jump back to footnote [28] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_29">
<sup>29</sup>. X. Shen, X. Tao, H. Gao, C. Zhou, and J. Jia. Deep automatic portrait matting. In <em>Proceedings of the European Conference on Computer Vision</em>, 2016. 1, 2<a href="#reffn_29" title="Jump back to footnote [29] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_30">
<sup>30</sup>. K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. <em>CoRR</em>, abs/1409.1556, 2014. 4<a href="#reffn_30" title="Jump back to footnote [30] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_31">
<sup>31</sup>. J. Sun, J. Jia, C.-K. Tang, and H.-Y. Shum. Poisson matting. In <em>ACM Transactions on Graphics (ToG)</em>, volume 23, pages 315–321. ACM, 2004. 1, 2<a href="#reffn_31" title="Jump back to footnote [31] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_32">
<sup>32</sup>. J. Wang and M. F. Cohen. Optimized color sampling for robust matting. In <em>2007 IEEE Conference on Computer Vision and Pattern Recognition</em>, pages 1–8. IEEE, 2007. 1, 2<a href="#reffn_32" title="Jump back to footnote [32] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_33">
<sup>33</sup>. J. Yang, B. Price, S. Cohen, H. Lee, and M.-H. Yang. Object contour detection with a fully convolutional encoder-decoder network. <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em>, 2016. 3<a href="#reffn_33" title="Jump back to footnote [33] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_34">
<sup>34</sup>. Y. Zheng and C. Kambhamettu. Learning based digital matting. In <em>2009 IEEE 12th International Conference on Computer Vision</em>, pages 889–896. IEEE, 2009. 5, 7, 8<a href="#reffn_34" title="Jump back to footnote [34] in the text."> &#8617;</a>
</blockquote>
<hr>
<p>由于单个图片排版较繁琐，方便起见直接从原论文截取图片，见谅。后面会研究下怎么能比较好的排版。</p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/深度抠图/" rel="tag"># 深度抠图</a>
              <a href="/tags/论文/" rel="tag"># 论文</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2019/03/03/Windows-10-安装-TensorFlow/" rel="prev" title="Windows 10 安装 TensorFlow">
      <i class="fa fa-chevron-left"></i> Windows 10 安装 TensorFlow
    </a></div>
      <div class="post-nav-item">
    <a href="/2019/04/05/Pop!_OS 与 Windows10 不同盘双系统启动问题/" rel="next" title="Pop!_OS 与 Windows10 不同盘双系统启动问题">
      Pop!_OS 与 Windows10 不同盘双系统启动问题 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  

  </div>


          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let activeClass = CONFIG.comments.activeClass;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#深度抠图"><span class="nav-number">1.</span> <span class="nav-text">深度抠图</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#摘要"><span class="nav-number">1.1.</span> <span class="nav-text">摘要</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-简介"><span class="nav-number">1.2.</span> <span class="nav-text">1. 简介</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-相关工作"><span class="nav-number">1.3.</span> <span class="nav-text">2. 相关工作</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-新的抠图数据集"><span class="nav-number">1.4.</span> <span class="nav-text">3. 新的抠图数据集</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-我们的方法"><span class="nav-number">1.5.</span> <span class="nav-text">4. 我们的方法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-抠图编码器-解码器阶段（Matting-encoder-decoder-stage）"><span class="nav-number">1.5.1.</span> <span class="nav-text">4.1. 抠图编码器-解码器阶段（Matting encoder-decoder stage）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2-抠图细化阶段（Matting-refinement-stage）："><span class="nav-number">1.5.2.</span> <span class="nav-text">4.2. 抠图细化阶段（Matting refinement stage）：</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-实验结果"><span class="nav-number">1.6.</span> <span class="nav-text">5. 实验结果</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#5-1-alphamatting-com-数据集"><span class="nav-number">1.6.1.</span> <span class="nav-text">5.1. alphamatting.com 数据集</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-2-Composition-1k-测试数据集"><span class="nav-number">1.6.2.</span> <span class="nav-text">5.2. Composition-1k 测试数据集</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-3-真实图像数据集"><span class="nav-number">1.6.3.</span> <span class="nav-text">5.3. 真实图像数据集</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-结论"><span class="nav-number">1.7.</span> <span class="nav-text">6. 结论</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#参考文献"><span class="nav-number">1.8.</span> <span class="nav-text">参考文献</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="YangGang" src="https://miao.su/images/2017/03/03/dark_deer302fb.png">
  <p class="site-author-name" itemprop="name">YangGang</p>
  <div class="site-description" itemprop="description">(ง •_•)ง</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">14</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">13</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">17</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/SamuelYG" title="GitHub → https://github.com/SamuelYG" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://twitter.com/yangg052" title="Twitter → https://twitter.com/yangg052" rel="noopener" target="_blank"><i class="fa fa-fw fa-twitter"></i>Twitter</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://instagram.com/samuelyg1996/" title="Instagram → https://instagram.com/samuelyg1996/" rel="noopener" target="_blank"><i class="fa fa-fw fa-instagram"></i>Instagram</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://weibo.com/5658432174" title="Weibo → https://weibo.com/5658432174" rel="noopener" target="_blank"><i class="fa fa-fw fa-weibo"></i>Weibo</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:yangg052@gmail.com" title="E-Mail → mailto:yangg052@gmail.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title">
      <i class="fa fa-fw fa-link"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://notes.iissnan.com/" title="https://notes.iissnan.com/" rel="noopener" target="_blank">IIssNan's Notes</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://milkice.me/" title="https://milkice.me/" rel="noopener" target="_blank">Milkice's IceBox</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://cdeveloper.cn/" title="http://cdeveloper.cn/" rel="noopener" target="_blank">cdeveloper</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://nfz.moe/" title="https://nfz.moe/" rel="noopener" target="_blank">neoFelhz</a>
        </li>
    </ul>
  </div>

      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">YangGang</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v3.8.0
  </div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">主题 – <a href="https://pisces.theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> v7.7.0
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>
  <script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script>
<script src="/js/schemes/pisces.js"></script>
<script src="/js/next-boot.js"></script>



  















  

  
      
<script type="text/x-mathjax-config">

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });

  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') {
          next = next.nextSibling;
        }
        if (next && next.nodeName.toLowerCase() === 'br') {
          next.parentNode.removeChild(next);
        }
      }
    });
  });

  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      element = document.getElementById(all[i].inputID + '-Frame').parentNode;
      if (element.nodeName.toLowerCase() == 'li') {
        element = element.parentNode;
      }
      element.classList.add('has-jax');
    }
  });
</script>
<script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML', () => {
    MathJax.Hub.Typeset();
  }, window.MathJax);
</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->

    

  

</body>
</html>
